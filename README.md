The repository applies different AI models to  detect AI generated text submitted by students.
The dataset used is from [Kaggle Competition](https://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-llm-detect-ai-generate) .
 

Our goal is to identify the model that can clearly distinguish if student has written the text or AI has generated the text. 
Dataset Used is found [here](data/) All the prompts given to achieve the below analysis can be found [here](prompts.ipynb).

The repository performs an analysis of text, aiming to identify a NLP Classification model that can detect AI generated text. 
Here's a breakdown of the key steps and insights:
1. Exploratory Data Analysis (EDA):
* Data Inspection: The code starts by examining the number of values for “generated” column in the essays_df dataset. Observed that  the data contains only text generated by students. So , added records from dataset generated by Derick. 
* Visualizations: Scatter plots and histograms are generated to visualize the relationship between various features (source, prompt) and the target variable (‘generated’). These visualizations help understand the correlation between the features
* Data Distribution: Examining the value counts for text_length to understand the distribution of text_length.
2. Data Preparation: 
* Feature Engineering: An ‘text length” column is created to check if that effects t
* Data Cleaning:
    * Columns with identifiers (prompt_id, text_id) are removed as they don't contribute to text.
* Data Type Conversion: All text columns are converted to string type.
3. Modeling:
* Train-Test Split: Data is split into 80% training and 20% testing sets.
* Normalization: Performed CountVectorization to remove stopwords of English and all “non-english” words from the text and count the word in the text.
* Feature Extraction:Performed TFIDF to extract features
* Stemming/Lemmitization 
* Baseline Classifier: Applied Dummy Classifier and got 86% of accuracy_score
* Logistic Regression: A Logistic regression model is trained with TFIDF Vectorization and received accuracy_Score of 76%
* Decision Tree: Decision Tree model is trained with TFIDF and received accuracy_score of 76%
* K Nearest Neighbors: K Nearest Neighbors model is trained with TFIDF and received accuracy_score of 99.9%
* Support Vector Machine:SVM model is trained with TFIDF and received accuracy_score of 99.8%
* Naive Bayes:This model is trained with TFIDF and received accuracy_score of  1

LatentDirichlet Allocation: Applied LDA embedding and applied Logistic Regression on the data and achieved accuracy _score of 1
NMF : Applied NMF embedding and applied Logistic Regression on the data and achieved accuracy _score of 1
4. Evaluation: 
Evaluated all these models on the basis of metrics such as F1Score, Precision, Recall and Accuracy Score. Also checked the Confusion Matrix to identify the number of TruePositives, TrueNegatives. 

![Screenshot 2025-05-20 at 11 04 45 AM](https://github.com/user-attachments/assets/0113bd56-1173-4fb7-81d2-1e87a956bdcb)
Based on all of these , Naive Bayes has outperformed all the other models.
