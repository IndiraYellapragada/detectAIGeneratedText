###  Text Classification to Detect AI Generated Text

The repository applies different AI models to  detect AI generated text from the submissions made by students.

 Our goal is to identify the model that can clearly distinguish if student has written the text or AI has generated the text. 

**Author** 
Indira Jyothi Yellapragada

#### Executive summary

**Project overview and goals:** The goal of this project is to identify more effective ways for detecting AI generated text from student submissions and evaluating the best model for detecting AI generated text. We will be training and tuning few binary text classification models to accurately classify texts as AI generated or not. Models will be able to predict, from future/unseen text, whether the text is original content of student or AI generated. We will then evaluate and compare the  models' performances to identify the best one. Lastly, we will draw insights from our analyses and recommmend areas to research and courses to undertake for future work in detecting AI generated text.  

#### Rationale
This is an important to encourage Students to submit their original work. Instructors have lot of extra work these days to identify students who plagiarize. This model should save time for the teachers.
#### Research Question

The question this project aims to answer is what is the best classification model for detecting if the submissions were made by students or LLM(Large Language Models such as ChatGPT)  has generated text.

#### Data Sources
The dataset used is from [Kaggle Competition](https://www.kaggle.com/code/alexia/kerasnlp-starter-notebook-llm-detect-ai-generate) .


Dataset Used is found [here](data/) 

#### Methodology

The repository performs an analysis of text, aiming to identify a NLP Classification model that can detect AI generated text. 
Here's a breakdown of the key steps and insights:
##### Exploratory Data Analysis (EDA):
**Data Inspection**: The code starts by examining the number of values for “generated” column in the essays_df dataset. Observed that  the data contains only text generated by students. So, added records from dataset generated by Derick. \
**Visualizations**: Scatter plots and histograms are generated to visualize the relationship between various features (source, prompt) and the target variable (‘generated’). These visualizations help understand the correlation between the features.\
**Data Distribution**: Examining the value counts for text_length to understand the distribution of text_length.
##### Data Preparation: 
**Feature Engineering**: An ‘text length” column is created to check if that helps in the classification.\
**Data Cleaning**: Columns with identifiers (prompt_id, text_id) are removed as they don't contribute to text.\
**Data Type Conversion**: All text columns are converted to string type.\
##### Modeling:
**Train-Test Split**: Data is split into 80% training and 20% testing sets.\
**Normalization**: Performed CountVectorization to remove stopwords of English and all “non-english” words from the text and count the word in the text.\
**Feature Extraction**:Performed TFIDF to extract features
**Baseline Classifier**: Applied Dummy Classifier and got 86% of accuracy_score.\
**Logistic Regression**: A Logistic regression model is trained with TFIDF Vectorization and received accuracy_Score of 76%.\
**Decision Tree**: Decision Tree model is trained with TFIDF and received accuracy_score of 76%.\
**K Nearest Neighbors**: K Nearest Neighbors model is trained with TFIDF and received accuracy_score of 99.9%.\
**Support Vector Machine**: SVM model is trained with TFIDF and received accuracy_score of 99.8%.\
**Naive Bayes**:This model is trained with TFIDF and received accuracy_score of  1.\
**LatentDirichlet Allocation:** Applied LDA embedding and applied Logistic Regression on the data and achieved accuracy _score of 1\
**NMF** : Applied NMF embedding and applied Logistic Regression on the data and achieved accuracy _score of 1\
#### Results
Evaluated all these models on the basis of metrics such as F1Score, Precision, Recall and Accuracy Score. Also checked the Confusion Matrix to identify the number of TruePositives, TrueNegatives. 
![Screenshot 2025-05-20 at 4 12 56 PM](https://github.com/user-attachments/assets/47ad3d49-dbbb-4e7a-978d-37fcc602106e)

Based on all of these , Naive Bayes has outperformed all the other models.

#### Next steps
We can explore more by identifying text specific to subject and list of words that appear only in AI generated text and those that appear only in original text.

#### Outline of project

- [Data](data/)
- [Link to notebook](prompts.ipynb)


##### Contact and Further Information


Indira Jyothi Yellapragada
Email: indirajyothi.yellapragada@gmail.com



